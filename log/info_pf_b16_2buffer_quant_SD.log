nohup: ignoring input
/root/miniconda3/envs/mixtral-offload/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/root/miniconda3/envs/mixtral-offload/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/root/miniconda3/envs/mixtral-offload/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/root/miniconda3/envs/mixtral-offload/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):
/root/miniconda3/envs/mixtral-offload/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/mixtral-offload/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
CUDA extension not installed.
CUDA extension not installed.
Some weights of the model checkpoint at /clk/model/Mistral-7B-Instruct-v0.1-GPTQ were not used when initializing MistralForCausalLM: ['model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.26.self_attn.q_proj.bias']
- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/miniconda3/envs/mixtral-offload/lib/python3.10/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
hqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Loading experts:   0%|          | 0/32 [00:00<?, ?it/s]Loading experts:   3%|▎         | 1/32 [00:00<00:10,  2.98it/s]Loading experts:   6%|▋         | 2/32 [00:00<00:10,  2.99it/s]Loading experts:   9%|▉         | 3/32 [00:01<00:09,  2.98it/s]Loading experts:  12%|█▎        | 4/32 [00:01<00:09,  3.00it/s]Loading experts:  16%|█▌        | 5/32 [00:01<00:09,  2.95it/s]Loading experts:  19%|█▉        | 6/32 [00:02<00:08,  3.01it/s]Loading experts:  22%|██▏       | 7/32 [00:02<00:08,  3.04it/s]Loading experts:  25%|██▌       | 8/32 [00:02<00:07,  3.06it/s]Loading experts:  28%|██▊       | 9/32 [00:02<00:07,  3.07it/s]Loading experts:  31%|███▏      | 10/32 [00:03<00:07,  3.06it/s]Loading experts:  34%|███▍      | 11/32 [00:03<00:06,  3.08it/s]Loading experts:  38%|███▊      | 12/32 [00:03<00:06,  3.12it/s]Loading experts:  41%|████      | 13/32 [00:04<00:06,  3.14it/s]Loading experts:  44%|████▍     | 14/32 [00:04<00:05,  3.16it/s]Loading experts:  47%|████▋     | 15/32 [00:04<00:05,  3.13it/s]Loading experts:  50%|█████     | 16/32 [00:05<00:05,  3.15it/s]Loading experts:  53%|█████▎    | 17/32 [00:05<00:04,  3.14it/s]Loading experts:  56%|█████▋    | 18/32 [00:05<00:04,  3.15it/s]Loading experts:  59%|█████▉    | 19/32 [00:06<00:04,  3.16it/s]Loading experts:  62%|██████▎   | 20/32 [00:06<00:03,  3.18it/s]Loading experts:  66%|██████▌   | 21/32 [00:06<00:03,  3.16it/s]Loading experts:  69%|██████▉   | 22/32 [00:07<00:03,  3.18it/s]Loading experts:  72%|███████▏  | 23/32 [00:07<00:02,  3.20it/s]Loading experts:  75%|███████▌  | 24/32 [00:07<00:02,  3.20it/s]Loading experts:  78%|███████▊  | 25/32 [00:08<00:02,  3.21it/s]Loading experts:  81%|████████▏ | 26/32 [00:08<00:01,  3.17it/s]Loading experts:  84%|████████▍ | 27/32 [00:08<00:01,  3.19it/s]Loading experts:  88%|████████▊ | 28/32 [00:08<00:01,  3.19it/s]Loading experts:  91%|█████████ | 29/32 [00:09<00:00,  3.17it/s]Loading experts:  94%|█████████▍| 30/32 [00:09<00:00,  3.05it/s]Loading experts:  97%|█████████▋| 31/32 [00:10<00:00,  2.80it/s]Loading experts: 100%|██████████| 32/32 [00:10<00:00,  2.91it/s]Loading experts: 100%|██████████| 32/32 [00:10<00:00,  3.08it/s]
Inputs 1
Batch Size: 1
Prefilling time: 4.629493713378906 seconds
Decoding time: 14.36393690109253 seconds
Decoding iterations: 8
Decoding time per iteration: 1.7954921126365662 seconds
Time per output token(TPOT): 1.7954921126365662 seconds
-------------------------------
Traceback (most recent call last):
  File "/root/AdapMoE/run_quant_SD.py", line 247, in <module>
    main()
  File "/root/AdapMoE/run_quant_SD.py", line 136, in main
    result = model.generate(
  File "/root/miniconda3/envs/mixtral-offload/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/envs/mixtral-offload/lib/python3.10/site-packages/transformers/generation/utils.py", line 1701, in generate
    return self.assisted_decoding(
  File "/root/miniconda3/envs/mixtral-offload/lib/python3.10/site-packages/transformers/generation/utils.py", line 4597, in assisted_decoding
    raise ValueError("The length of the input ids in assistant inputs should be 1 or 2")
ValueError: The length of the input ids in assistant inputs should be 1 or 2
